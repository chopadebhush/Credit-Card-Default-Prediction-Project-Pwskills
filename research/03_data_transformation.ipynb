{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\SIR\\\\Credit-Card-Default-Prediction-Project-Pwskills\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\SIR\\\\Credit-Card-Default-Prediction-Project-Pwskills'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    transformed_train_dir: Path\n",
    "    transformed_test_dir: Path\n",
    "    scaler_file: Path\n",
    "    train_file: Path\n",
    "    test_file: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccdp.constants import *\n",
    "from ccdp.utils.common import read_yaml, create_directories\n",
    "#from pathlib import Path\n",
    "#from src.ccdp.entity.data_transformation import DataTransformationConfig\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    # Data Transformation Configuration\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.transformed_train_dir, config.transformed_test_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            transformed_train_dir=Path(config.transformed_train_dir),\n",
    "            transformed_test_dir=Path(config.transformed_test_dir),\n",
    "            scaler_file=Path(config.scaler_file),\n",
    "            train_file=Path(config.train_file),\n",
    "            test_file=Path(config.test_file)\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from pathlib import Path\n",
    "from ccdp.logging import logger\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def transform_data(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Identify numerical and categorical columns\n",
    "        numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        # Apply standard scaling to numerical features\n",
    "        X[numerical_columns] = self.scaler.fit_transform(X[numerical_columns])\n",
    "        \n",
    "        # Apply label encoding to categorical features\n",
    "        for column in categorical_columns:\n",
    "            encoder = LabelEncoder()\n",
    "            X[column] = encoder.fit_transform(X[column])\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def save_transformed_data(self, X: pd.DataFrame, y: pd.Series, file_name: str, is_train: bool = True):\n",
    "        output_dir = self.config.transformed_train_dir if is_train else self.config.transformed_test_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        transformed_df = pd.concat([X, y], axis=1)\n",
    "        file_path = output_dir / file_name\n",
    "        transformed_df.to_csv(file_path, index=False)\n",
    "        logger.info(f\"Transformed data saved to {file_path}\")\n",
    "\n",
    "    def transform_and_save(self):\n",
    "        # Load train and test datasets\n",
    "        train_df = pd.read_csv(self.config.train_file)\n",
    "        test_df = pd.read_csv(self.config.test_file)\n",
    "\n",
    "        # Separate features and target\n",
    "        X_train = train_df.drop(columns=[\"default\"])\n",
    "        y_train = train_df[\"default\"]\n",
    "        \n",
    "        X_test = test_df.drop(columns=[\"default\"])\n",
    "        y_test = test_df[\"default\"]\n",
    "        \n",
    "        # Transform train and test data\n",
    "        logger.info(\"Starting data transformation for training data...\")\n",
    "        X_train_transformed = self.transform_data(X_train)\n",
    "        \n",
    "        logger.info(\"Starting data transformation for test data...\")\n",
    "        X_test_transformed = self.transform_data(X_test)\n",
    "        \n",
    "        # Save the transformed data\n",
    "        self.save_transformed_data(X_train_transformed, y_train, \"transformed_train.csv\", is_train=True)\n",
    "        self.save_transformed_data(X_test_transformed, y_test, \"transformed_test.csv\", is_train=False)\n",
    "\n",
    "        # Save the scaler used during the transformation\n",
    "        self.save_scaler()\n",
    "        \n",
    "    def save_scaler(self):\n",
    "        os.makedirs(os.path.dirname(self.config.scaler_file), exist_ok=True)\n",
    "        with open(self.config.scaler_file, \"wb\") as scaler_file:\n",
    "            pickle.dump(self.scaler, scaler_file)\n",
    "        logger.info(f\"Scaler saved to {self.config.scaler_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-20 23:24:15,937: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-08-20 23:24:15,945: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-08-20 23:24:15,949: INFO: common: created directory at: artifacts]\n",
      "[2024-08-20 23:24:15,953: INFO: common: created directory at: artifacts/data_transformation/train]\n",
      "[2024-08-20 23:24:15,955: INFO: common: created directory at: artifacts/data_transformation/test]\n",
      "[2024-08-20 23:24:16,175: INFO: 409266770: Starting data transformation for training data...]\n",
      "[2024-08-20 23:24:16,242: INFO: 409266770: Starting data transformation for test data...]\n",
      "[2024-08-20 23:24:17,757: INFO: 409266770: Transformed data saved to artifacts\\data_transformation\\train\\transformed_train.csv]\n",
      "[2024-08-20 23:24:18,018: INFO: 409266770: Transformed data saved to artifacts\\data_transformation\\test\\transformed_test.csv]\n",
      "[2024-08-20 23:24:18,023: INFO: 409266770: Scaler saved to artifacts\\data_transformation\\scaler.pkl]\n",
      "[2024-08-20 23:24:18,028: INFO: 2665763243: Data transformation pipeline completed successfully.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Configuration for Data Transformation\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    \n",
    "    # Initialize the DataTransformation component\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    # Perform the data transformation and save the results\n",
    "    data_transformation.transform_and_save()\n",
    "    \n",
    "    logger.info(\"Data transformation pipeline completed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Pipeline failed due to: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
